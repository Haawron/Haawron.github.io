---
title: 01. LeNet
category: Deep Learning
excerpt: |
  [Deep Learning] 01. LeNet
feature_text: |
  <h1 style="color:white;font-weight:bold;font-style:normal;font-size:66px">
  01. LeNet
  </h1>
feature_image: "https://unsplash.it/1200/400?image=1048"
---

# Yann Lecun
Yann Lecun은 CNN을 처음 개발했고, 전 세계 딥러닝 4대 천왕 중 한 명으로 LeNet을 시작으로 지금까지
딥러닝에 역사적인 기여를 했습니다. 현재는 페이스북에서 AI 기반 로봇을 연구 중이라고 합니다.(2018.1)  

LeNet은 LeNet - 1부터 LeNet - 5까지 5종류가 있지만 1과 5가 가장 유명합니다. 이 두 개에 대해서
알아봅시다 ㅎㅎ  

## LeNet - 1 (1989)
기존에는 Fully Connected Network(FCN)만을 사용하고 있었습니다.
하지만 FCN은 개체가 이동하거나 회전하는 등 아주 살짝만 변형이 되어도 완전히 다른 이미지로
인식되어 분명한 한계가 있었습니다. 그래서 학습도 느렸고 결과도 별로 좋지 않았어요.  
그런데 2D Convolution을 활용해서 국소적인 특징을 추출해내는 것으로 이것을 해결할 수 있다는 것을
알아냈습니다. 그렇게 해서 2D Convolution과 pooling(논문에서는 squashing function)을
적절히 조합해 최초의 CNN인 LeNet-1이 탄생했습니다!!  


### 구조
{% include figure.html image="https://i.imgur.com/YpnFQAE.png" position="center" %}  
이미지를 구글에서 가져왔는데 <span style="color:red">마지막 Layer는 Convolution이 아니라 Fully Connected입니다.</span>  
제대로 만들어서 다시 올릴거에요.  
{% include figure.html image="https://i.imgur.com/AqFyYyO.jpg" position="center" %}  
오래된 구조라 패치로 비교적으로 큰 5x5를 쓰는 것을 볼 수 있습니다. 요즘에는 거의 의무적으로
3x3만 쓰는 추세입니다. 이 구조는 약 30년이 지난 지금까지 크게 변하지 않고 쓰이고 있습니다.  
1989년이 벌써 30년이 다 되어간다니 살짝 소름이네요 ㅎㅎ


### 특징
특징 몇 개만 알아보면
- 위쪽으로 갈수록 더 global한 feature를 학습한다. (어떻게 feature가 더 작아지는데 global한 내용을 학습하는지는 뒤의 ZFNet에서 다룸)
- FCN보다 parameter수가 매우 적다. bias를 제외하고 3220개 밖에 되지 않는다.
- max-pooling으로 강한 자극을 취했다. 논문에서는 자극을 요약(summarize)한다고 표현했다.
- 전체 이미지에 같은 kernel을 써서 shared weight를 적용했다.
- 이 때까지의 다른 모델보다 훨씬 효과가 좋았다.
- <span style='color:red'>사실 훨씬 더 크게 만들 수 있었는데 컴퓨팅 파워의 한계로 parameter수에 제한이 컸다.</span>

대충 이 정도 있습니다. 이 구조면 지금이야 1060으로 몇 분만에 학습할 수 있지만 그 시절이면
286 CPU를 쓰고 있었을텐데 상상만해도 끔찍합니다.
[위키]에서 찾아보니까 클럭속도가 4~12.5 MHz라네요...ㅋㅋ  

찾았어요!!  
{% include figure.html image="https://i.imgur.com/XsaHlwl.jpg" position="center" caption="SUN SPARCstation 1 - 출처 : 위키" %}   
이 컴퓨터에서 3일동안 돌렸다고 합니다. 다행히도 클럭은 20MHz였네요 ㅋㅋ  

워낙 오래돼서 실용성은 적지만 최초라는 점에서 학문적인 의미는 크다고 할 수 있습니다.  
그리고 이 연구에 사용된 데이터는 손글씨로 쓴 우편번호들인데, 지금은 이것들이 정제되어 MNIST라는 데이터셋으로 딥러닝의 Hello world 같은 개념으로 활용되고 있습니다. 초보자 입문용일 뿐만 아니라
용량도 작고 학습도 쉬워서 간단한 모델 성능 테스트 해볼 때도 유용합니다.   


## LeNet - 5 (1998)
위 구조에서 발전한 5 버전입니다.

### 구조
{% include figure.html image="https://i.imgur.com/yix0Bte.png" position="center" %}  
{% include figure.html image="https://i.imgur.com/tR97gXE.jpg" position="center" %}  

구조가 상당히 커진 것을 볼 수 있는데요. 얘는 어디서 학습시켰을지 궁금해지네요.  
{% include figure.html image="https://i.imgur.com/7jwGuM8.jpg" position="center" caption="Silicon Graphics Origin 2000" %}  
바로 여기서 학습시켰다고 합니다. 무시무시하네요. 크기가 커진만큼 클럭이 10배 증가한 200MHz가
되었구요. 버전 1과 똑같이 3일 걸렸다고 하네요.  

### 특징
특징을 보면 구조뿐만 아니라 여러가지 부분에서 세세하게 신경썼음을 알 수 있습니다.
- input이미지를 위/아래/양옆 2px씩 빈 공간을 추가하여 크기를 32x32로 늘렸다.
Convolution이 이미지 끄트머리에서도 유효하게 되면서 좀 더 detail한 부분을 표현할 수 있다.
이 개념은 후에 padding으로 발전한다.
- parameter는 6만 개로 거의 20배가 되었지만 성능은 훨씬 더 좋다. (FCL 하나가 추가될 때마다 기하급수적으로 늘어난다.)
- (\*)C2(첫 번째 그림에서 두 번째 Convolution) 할 때의 각 output feature map은 미리 정한 3개의 input feature map으로만 진행했다. 순서쌍은 다음 표와 같다.  
{% include figure.html image="https://i.imgur.com/GoChfx4.png" position="center" %}
- 단계별 feature map에서 shift invariance를 확인할 수 있다.

{% include figure.html image="http://yann.lecun.com/exdb/lenet/gifs/asamples.gif" caption="LeNet-5" position="center" %}


## 한계
사실 인공신경망은 이 때 가장 절정이었습니다. 연구는 포화상태였고 어느 학문이나 그렇듯 이후에
자연스럽게 암흑기가 찾아오게되죠. 아니나 다를까 LeNet을 비롯한 인공신경망 기술은 한계를 보이기
시작했고 통계에 기반한 다른 머신러닝 기법에 묻히게 됩니다.


[위키]:https://ko.wikipedia.org/wiki/%EC%9D%B8%ED%85%94_80286#역사
