---
title: 02. AlexNet
category: Deep Learning
excerpt: |
  [Deep Learning] 02. AlexNet
feature_text: |
  <h1 style="color:white;font-weight:bold;font-style:normal;font-size:66px">
  02. AlexNet
  </h1>
feature_image: "https://unsplash.it/1200/400?image=1048"
---

# Alex Khrizevsky (2012)

얀 르쿤의 LeNet으로 딥러닝은 절정에 이르렀지만 이후 여러가지 문제점들이 대두되면서 암흑기가 찾아오게 된다.  
대표적으로 Gradient vanishing 문제와 local minima 문제가 있다.  
많은 Layer를 거쳐 Backpropagation을 하게 되면 sigmoid의 도함수가 계속 곱해져 gradient가 0에 빠르게
수렴(vanishing)하게 된다. 그래서 low-level layer의 학습속도가 현저히 느려지게 된다.  
그리고 gradient에 기반해서 모델을 학습하게 되면 local minima에 쉽게 빠지게 되는데 이를 극복하지 못했다.  

하지만 2006년도에 인공신경망 외길을 걷던 제프리 힌튼이
[A fast learning algorithm for deep belief nets]이라는 논문에서
먼저 학습(pre-training)시킨 모델을 쌓아 올리는 기법으로 괜찮은 결과를 보여주었다.  
이를 계기로 딥러닝 연구는 다시 활기를 찾았고 2012년 ILSVRC[^1]에서 딥러닝 모델이 압도적으로 우승하게 된다.

## 2012 ILSVRC 우승작
- 당시 SuperVision이라는 이름으로 압도적 1등을 하며 딥러닝 붐을 일으켰다.
- 연산시 GPU를 활용하여 의미있는 결과를 얻었다.
- LeNet-5와 맥락은 크게 다르지는 않지만 다양한 pre-process를 연구해보고 적용했다.
- Convolutional Layer 5개, Fully Connected Layer 3개로 구성했다.
- Conv Layer를 연속적으로 적용했다.
- Output Layer에 softmax를 홣용했다.
- 65만 개의 뉴런, 6천만 개의 parameter, 6억 3천만 개의 connection으로 이루어져 있다.
- GTX 580 2개로 1주일 동안 학습시켰다.



[^1]: ImageNet[^2] Large Scale Visual Recognition Challenge  
[^2]: 이미지 인식 모델의 성능 검증의 기준이 되다시피하는 데이터셋  
[A fast learning algorithm for deep belief nets]: (https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
